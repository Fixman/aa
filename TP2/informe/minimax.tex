\subsection{Performance against Minimax}

Durante el TP se desarrolló una implementación de MinimaxPlayer que obtiene muy buenos resultados.
Utiliza una ventana de 3 movimientos a futuro, premiando los escenarios ganadores con un valor de 1,
castigando los escenarios perdedores con -1 y devolviendo 0 para los escenarios en donde la ventana alcanza
un valor de 0 y ningún jugador ha ganado o perdido.

Dicha implementación fue testeada contra un jugador random alcanzando un niveles de efectividad muy altos.
Debido a las características recursivas del algoritmo el algoritmo toma un tiempo no despreciable en ejecutarse,
por lo que experimentamos hasta una cantidad de juegos N con un valor máximo de 100.000

A continuación se incluye una tabla en donde mostramos la efectividad de un QLearn Player compitiendo contra
un Minimax player para distinta cantidad de juegos N.

Los parámetros del QLearn Player fueron fijados con los siguientes valores:
$\epsilon$ = 0.2, $\alpha$ = 0.3, $\gamma$ = 0.9

Los resultados están expresados en función de la cantidad de victorias del jugador QLearn sobre el total de partidos
jugados

~

\begin{center}
\begin{tabular}{c c}
	\toprule
	\textbf{\#Games} & \textbf{QLearn victories} \\
	\midrule
	0 & 0.01 \\
	1000 & 0.013 \\
	5000 & 0.0136 \\
	10000 & 0.0121 \\
	20000 & 0.01205 \\
	40000 & 0.012775 \\
	100000 & 0.0117 \\
	\bottomrule
\end{tabular}
\end{center}

~

Según los resultados podemos concluir que la cantidad de partidos jugados no son suficientes como para que el
algoritmo de QLearn desarrolle una política $\pi$ efectiva. Por el contrario, la ventana de valor 3
le es suficiente al Minimax player para poder anticipar y dirigir sus movimientos a escenarios ganadores y poder
evitar los perdedores.
